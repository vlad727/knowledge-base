# Main description for test k8s cluster  
1 master node  + 1 worker node
Kubernetes version 1.30
Container runtime - crio
Cni - cilium
Ingress Controllers  - nginx or gateway api
Cluster OS - rpm based distrib

Note:
Before installation, check capabilty kernel version and kubernetes, also container runtime too
from official site https://cri-o.io/
KUBERNETES_VERSION=v1.32
CRIO_VERSION=v1.32

## Links with manuals
## https://www.youtube.com/watch?v=Vw0c3ZaR9uI
## https://blog.andreev.it/2023/10/install-kubernetes-with-cri-o-and-cilium-on-rocky-linux-9/
## https://vaiti.io/kak-ustanovit-klaster-kubernetes-s-cri-o-v-kachestve-container-engine/

################################################################################################################################
## Timeweb cloud, settings for virtual machines with private ip
## get name for private interface
nmcli c s
NAME                UUID                                  TYPE      DEVICE
System eth0         5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03  ethernet  eth0
Wired connection 1  43a177d0-cd70-3e4c-b755-72dce7e2351c  ethernet  eth1

## set ip address with nmcli 
sudo nmcli connection modify "Wired connection 1" ipv4.method manual ipv4.addresses 192.168.82.2/24 connection.autoconnect yes

## activate connection 
sudo nmcli connection up "Wired connection 1"

Note:
Private networker already binded to virtual during creation
################################################################################################################################
## Install k8s stage

1.
### Set hostname for node
sudo hostnamectl set-hostname master.local
sudo hostnamectl set-hostname worker.local

2.
### Manage users
useradd  vlad
passwd vlad
usermod -a -G wheel vlad
echo 'vlad ALL=(ALL) NOPASSWD:ALL' > /etc/sudoers.d/nopassword # another way use command "| sudo tee /etc/sudoers.d/nopassword" if you are undere user vlad
cat /etc/sudoers.d/nopassword

Note:
set password or use only key
above commands for both nodes
if you add vlad to nopassword file, user vlad may exec command without ask password only use comand sudo


3.
### Add new records to file hosts
vim  /etc/hosts and add lines below:
192.168.82.2 master.local
192.168.82.5 worker.local

Note:
If you want to check your host with util host ("host" utility, used dns and ignore file /etc/hosts) 
instead "host" use "getent"
getent hosts master.local

4.
### Install packages
sudo yum install net-tools curl wget bind-utils -y

Note:
package ca-certificates installed by default for Almalinux
but if you use deb based os, need to install packages below:
sudo apt-get install ca-certificates app-transport-https curl

5.
### Disable selinux 
sudo sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux
sudo reboot

6.
### Disable swap
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

7.
### Add modules, for new operating systems, such as almalinux etc.
sudo tee /etc/modules-load.d/modules.conf <<EOF
overlay
br_netfilter
EOF

Note:
For old operating systems, start overlay and netfilter after reboot nodes use command below:
echo "overlay" >> /etc/modules
echo "br_netfilter" >> /etc/modules
File may has another name, not only modules.conf, also it may has a name crio.conf 

8.
### Sysctl params, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system

Note: 
# Additional sysctl settings for k8s
net.ipv4.tcp_max_syn_backlog=100000
net.ipv4.tcp_tw_reuse=1
net.core.somaxconn=65535
net.core.netdev_max_backlog=100000
fs.file-max=500000
net.ipv4.ip_forward=1
net.ipv4.ip_local_port_range=5000 60999
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.bridge.bridge-nf-call-arptables=1

# to switch off ipv6, settings below forbidden to use ipv6 for kubeadm
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1



9.
### Add repos k8s and crio
#### link with repo examples https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/change-package-repository/

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.35/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.35/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl kubernetes-cni
EOF

Note:
If you use deb based os, repo will be for deb
To check repo use command below:
dnf repolist or yum repolist
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni <<< special flag which one allow you to exclude such packages from update/install process, if run yum update -y
(что бы случайно не обновить пакеты после yum update -y)
Or create file by manual
sudo vi /etc/yum.repos.d/kubernetes.repo
The URL used for the Kubernetes package repositories is not limited to pkgs.k8s.io, it can also be one of:
pkgs.k8s.io
pkgs.kubernetes.io
packages.kubernetes.io

Add repo for crio
VERSION=1.28 <<< this the last version from opensuse 
sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/CentOS_8/devel:kubic:libcontainers:stable.repo
sudo curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:${VERSION}.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:${VERSION}/CentOS_8/devel:kubic:libcontainers:stable:cri-o:${VERSION}.repo

Note:
devel:kubic:libcontainers:stable является стандартным названием пространства имен проекта OpenSUSE Kubic. Оно обозначает следующую структуру:
- devel: пространство разработки (development)
- kubic: проект OpenSUSE Kubic
- libcontainers: библиотека контейнеров
- stable: стабильная ветка релизов

10.
### kubelet, kubeadm, kubectl and crio installation stage

sudo dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
sudo dnf install -y cri-o cri-tools

sudo systemctl enable --now crio && systemctl status crio
sudo systemctl enable --now kubelet && systemctl status kubelet

Note:
"enable --now" will start daemon and add it to autostart 
Repeat above command for both nodes
--disableexcludes=kubernetes <<< allow you to disable your exclude for such packages from your kubernetes.repo

11.
### Init stage for master node
sudo kubeadm init --control-plane-endpoint="master.local:6443" --pod-network-cidr=10.244.0.0/16
# You’ll get an output that says how to join the nodes, something like kubeadm join master… + a token

# if get any an erros like below:
A control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/crio/crio.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/crio/crio.sock logs CONTAINERID'

# need to troubleshoot an issue and resolve it, after reset init with command below:
sudo kubeadm reset -f 
sudo rm -rf /etc/kubernetes/ /var/lib/kubelet/ /var/lib/etcd/ /var/lib/containerd/
sudo systemctl daemon-reload

Note:
Command above, will create certificates and configuration files with pod network 
all ip addresses with sych mask /16: 65 536.
adresses for host: 65 534.
reserved (not for hosts): 2 address (network and broadcast).

For debug init use --v=5 or higher

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

12.
### Cilium install 
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
cilium install --version 1.14.2

13. 
### Join worker node
sudo kubeadm join master.homelab.local:6443 --token t2yir0.7n49rn6r57msy4j4 \
        --discovery-token-ca-cert-hash sha256:563360fcf60c49be91cdcf6486a4954c579a80c54503127ee0682ab8f86ec840

Main Notes:

Errors and Warnings

Warning messages after init 
[preflight] Running pre-flight checks
	[WARNING ContainerRuntimeVersion]: You must update your container runtime to a version that supports the CRI method RuntimeConfig. Falling back to using cgroupDriver from kubelet config will be removed in 1.36. For more information, see https://git.k8s.io/enhancements/keps/sig-node/4033-group-driver-detection-over-cri
	[WARNING SystemVerification]: kernel release 5.14.0-611.13.1.el9_7.x86_64 is unsupported. Supported LTS versions from the 5.x series are 5.4, 5.10 and 5.15. Any 6.x version is also supported. For cgroups v2 support, the recommended version is 5.10 or newer

Issue:
[WARNING SystemVerification]: kernel release 5.14.0-611.16.1.el9_7.x86_64 is unsupported. Supported LTS versions from the 5.x series are 5.4, 5.10 and 5.15. Any 6.x version is also supported. For cgroups v2 support, the recommended version is 5.10 or newer

Resolution:
sudo dnf install http://www.elrepo.org/elrepo-release-9.el9.noarch.rpm -y
sudo dnf --enablerepo="elrepo-kernel" list available | grep kernel
sudo dnf --enablerepo="elrepo-kernel" install kernel-ml
sudo dnf list installed kernel*
uname -r
sudo grub2-mkconfig -o /boot/grub2/grub.cfg

!!! If you node has 2 ip addresses public and pivate you need to install dnsmasq !!!
sudo dnf install dnsmasq -y
echo -en "address=/master-1/192.168.82.2\\naddress=/worker-1/192.168.82.5" | sudo tee -a /etc/dnsmasq.conf 

sudo systemctl start dnsmasq
sudo systemctl enable dnsmasq
	
# create file local and add nameserver
sudo mkdir root:root /etc/resolv.conf.d
echo "nameserver 127.0.0.1" | sudo tee /etc/resolv.conf.d/local
sudo chown root:root  /etc/resolv.conf.d/local
sudo chmod 644  /etc/resolv.conf.d/local
sudo systemctl restart NetworkManager

!!! after settings above utility "host" still will ingnore file /etc/hosts and will use file /etc/resolv.conf
so you need to add your own dnsmanq to /etc/resolv.conf (do not edit manualy, it managed by NetworkManager)
do settings below:

# check your interfaces
nmcli connection show --active
NAME                UUID                                  TYPE      DEVICE
System eth0         5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03  ethernet  eth0
Wired connection 1  43a177d0-cd70-3e4c-b755-72dce7e2351c  ethernet  eth1
# switch off auto-dns from your cloud provider
sudo nmcli connection modify "System eth0" ipv4.ignore-auto-dns yes
sudo nmcli connection modify "Wired connection 1" ipv4.ignore-auto-dns yes


# worker node 
echo "nameserver 192.168.82.2" | sudo tee /etc/resolv.conf.d/local
sudo systemctl restart NetworkManager

# chrony how to 
sudo yum install chrony  -y
sudo vim /etc/chrony.conf

# comment 2.almalinux and ru ntp
pool ru.pool.ntp.org iburst

# set time zone
sudo timedatectl set-timezone Europe/Moscow

# get all stings/line from file which one not empty and not start with #
cat /etc/crio/crio.conf | grep -v '^$' | grep -v '^#'
Когда мы объединяем эти два символа подряд (^$), это фактически означает "строку, которая начинается и сразу заканчивается", то есть пустую строку.

# crio setings from already worked cluster 
[crio]
root = "/var/lib/containers/storage"
runroot = "/var/run/containers/storage"
log_dir = "/var/log/crio/pods"
version_file = "/var/run/crio/version"
version_file_persist = "/var/lib/crio/version"
[crio.api]
listen = "/var/run/crio/crio.sock"
stream_address = "127.0.0.1"
stream_port = "10010"
stream_enable_tls = false
stream_tls_cert = ""
stream_tls_key = ""
stream_tls_ca = ""
grpc_max_send_msg_size = 16777216
grpc_max_recv_msg_size = 16777216
[crio.runtime]
default_runtime = "runc"
no_pivot = false
decryption_keys_path = "/etc/crio/keys/"
conmon = "/usr/local/bin/conmon"
conmon_cgroup = "system.slice"
conmon_env = [
	"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
]
default_env = [
]
selinux = true
seccomp_profile = ""
cgroup_manager = "systemd"
default_capabilities = [
	"CHOWN",
	"DAC_OVERRIDE",
	"FSETID",
	"FOWNER",
	"NET_RAW",
	"SETGID",
	"SETUID",
	"SETPCAP",
	"NET_BIND_SERVICE",
	"SYS_CHROOT",
	"KILL",
]
default_sysctls = [
]
additional_devices = [
]
hooks_dir = [
	"/usr/share/containers/oci/hooks.d",
]
default_mounts = [
]
log_size_max = -1
log_to_journald = false
container_exits_dir = "/var/run/crio/exits"
container_attach_socket_dir = "/var/run/crio"
bind_mount_prefix = ""
read_only = false
log_level = "info"
log_filter = ""
uid_mappings = ""
gid_mappings = ""
ctr_stop_timeout = 30
manage_ns_lifecycle = false
namespaces_dir = "/var/run"
pinns_path = ""
[crio.runtime.runtimes.runc]
runtime_path = "/usr/bin/runc"
runtime_type = "oci"
runtime_root = "/run/runc"
privileged_without_host_devices = false
allowed_annotations = []
[crio.image]
default_transport = "docker://"
global_auth_file = "/etc/crio/config.json"
pause_image = "registry.apps.k8s.ose-prod.solution.sbt/kubernetes/pause:3.8"
pause_image_auth_file = ""
pause_command = "/pause"
signature_policy = ""
image_volumes = "mkdir"
[crio.network]
network_dir = "/etc/cni/net.d/"
plugin_dirs = [
	"/opt/cni/bin",
	"/usr/libexec/cni",
]
[crio.metrics]
enable_metrics = true
metrics_port = 9090


   
