# Most common questions
1. Git rebase, branches, commit etc.
2. Docker cmd/entrypoint etc.
3. «Bootstrapping process» (буквально — «процесс подтягивания себя за шнурки») это начальный этап запуска чего‑либо с минимальными внешними ресурсами, когда система или проект «заводит себя сам» за счёт внутренних механизмов.

# Interview plan 
Git - 
Terraform - ?
Monitroing - Grafana, Prometheus, Victoria Metrics, AlertManager, Tracing 
Ansible - what is role, playbook, compare with chief and puppet 
Kubernetes - tolaration and taints, node port, services
##########################################################################################################################################
# Docker
Do small image 
.dockerignore
multistage

# what is containers
cgroups, namespaces etc.


##########################################################################################################################################
# About LLM and AI
Don't forget about role
Example you write promt and tell "you are devops ingener" <<< it is role

##########################################################################################################################################
JWT (JSON Web Token) — это открытый стандарт (RFC 7519) для безопасной передачи данных между сторонами в формате JSON. Используется преимущественно для аутентификации и авторизации в клиент‑серверных приложениях.

Ключевые особенности
Компактность. Токены представляют собой строку, которую легко передавать через URL, HTTP‑заголовки или тело запроса.

Самодостаточность. Все необходимые данные (например, информация о пользователе) содержатся внутри токена — серверу не нужно обращаться к базе данных для проверки.

Безопасность. Токены подписываются секретным ключом, что гарантирует их целостность и подлинность. При использовании алгоритмов HMAC или RSA данные также шифруются.


##########################################################################################################################################
# CI/CD
CI
Result for CI it's artifact
build > test > artifact (artifact with tag and it's )
PIPLINE get code > Sonar > set tags for images, helm charts > build or without build > functionality test ()
CD 
It's already deploy application to stage, test or production

##########################################################################################################################################
# Linux 

##########################################################################################################################################
# Kubernetes
Main questions about Kubernets 
1. `kubectl apply -f my-deployment.yaml`
   Пользователь отправляет команду через CLI (kubectl), указывая файл конфигурации YAML.

2. Запрос отправляется в API-сервер Kubernetes
   Команда передается на сервер API Kubernetes, который является точкой входа для всех операций управления кластером.

3. Проверка аутентификации и авторизации (AAA)
   API-сервер сначала проверяет подлинность пользователя (Authentication). Затем выполняется проверка прав доступа, чтобы убедиться, что запрашиваемый ресурс разрешен данному пользователю (Authorization).

4. Валидация ресурса
   Если доступ подтвержден, API-сервер проверяет корректность описания ресурса согласно спецификации Kubernetes (Validation).

5. Создание записи в etcd
   После успешной проверки, новая версия Deployment сохраняется в распределенной хранилище состояния etcd.

6. Controller Manager замечает изменения
   Контроллер контроллера Deployment постоянно следит за изменениями в etcd и обнаруживает новое состояние Deployment.

7. Контроллер создает реплики Pods
   Controller manager вызывает создание необходимых Pod'ов для достижения желаемого количества экземпляров приложения (Deployment spec).

8. Scheduler назначает Pod на Node
   Scheduler выбирает подходящую ноду (Node) для размещения каждого нового Pod'а исходя из доступности ресурсов и ограничений размещения.

9. Kubelet запускает контейнеры на выбранной ноде
   Kubelet на конкретной ноде получает инструкции от API-сервера и инициирует запуск контейнера (через Docker/Cri-o или другой runtime) с заданными параметрами.

kubectl → API Server → Authentication & Authorization → Validation → etcd → Controller Manager → Scheduler → Kubelet → Container Runtime

   # Scheduler and kubelet
   Изменение статуса Pod
   Как только Scheduler назначит Pod определённому узлу, этот факт записывается обратно в хранилище (etcd), и статус Pod меняется с Pending на Scheduled.
   
   Kubelet активируется на узле
   Компонент kubelet, работающий непосредственно на каждом узле, периодически запрашивает у API-сервера список новых назначенных Pod’ов. Обнаружив новые задания, kubelet приступает к процессу их исполнения.

   Kubelet запускает контейнеры

   - Проверяет наличие образа контейнера, указанного в описании Pod (если образ отсутствует, скачивает его);
   - Создаёт сетевое пространство (Network Namespace) и настраивает сети (например, CNI);
   - Запускает сам контейнер внутри выбранного пространства имен (namespace) через контейнерный движок (Docker, containerd и др.).

   # Docker, crio and containerd 
   Правильное обобщающее название для Docker, containerd и крио — это контейнерные рантаймы (или контейнерные среды выполнения)

   1. Scheduler определяет оптимальную ноду для Pod и записывает эту информацию в etcd.
   2. Kubelet на соответствующей ноде замечает появление нового Pod'а.
   3. Kubelet взаимодействует с контейнерным рантаймом (например, Docker или containerd):
   - Скачивается нужный образ (при необходимости).
   - Создаются необходимые структуры для запуска (сетевые настройки, PID namespace и т.д.)
   - Контейнеры стартуют.
   
   # About container run time
   - Container: Контейнер (англ. container) — стандартизированная единица программного обеспечения, включающая приложение вместе с зависимостями (библиотеки, конфигурационные файлы и т.д.). Основная цель            контейнеров — упрощение упаковки и распространения приложений.

   - Runtime: Окружение выполнения (англ. runtime environment) — среда, обеспечивающая выполнение программы или контейнера, включая систему виртуализации, процессы изоляции и управление ресурсами.
     То есть, container runtime — это окружение, инфраструктура и инструменты, позволяющие создавать, запускать и управлять контейнерами, обеспечивая изоляцию процессов, управление ресурсами и взаимодействие с      хост-машиной.
### nodePort
Issue:
при использовании сервиса с типом NodePort, запросы отправляются на IP конкретного узла (ноды), и если Pod перемещается на другую ноду,
вам придётся вручную отслеживать IP адреса и перенаправлять трафик туда
Resolution:
Даже если под переехал на другую ноду, вы по‑прежнему можете:
стучаться в IP_любой_ноды:30005;
получать ответ от пода (Kubernetes маршрутизирует трафик).

Так же есть механизм уменьшения "хопов"
Local Traffic Policy (service.kubernetes.io/internal-traffic-policy)
Суть: заставляет kube-proxy маршрутизировать трафик только на поды, запущенные на той же ноде, если они есть.
Эффект: если под есть на ноде‑входа, трафик идёт напрямую, минуя другие ноды.

External Traffic Policy (externalTrafficPolicy: Local)
Суть: аналогична internal-traffic-policy, но для внешнего трафика.
Эффект: трафик с NodePort обрабатывается только если под запущен на этой ноде.

LoadBalancer с Native Routing
Суть: облачные балансировщики (AWS NLB, GCP Network Load Balancer) могут маршрутизировать трафик напрямую на IP подов, минуя kube-proxy.
Эффект: сокращение хопов до:
Клиент → LoadBalancer → Нода с подом



Благодаря правилам iptables на ноде B, запрос автоматически перенаправляется на соответствующую ноду A, где размещён реальный Pod.
Сервис успешно обрабатывает запрос и возвращает ответ клиенту.

### pause image
невозможно запустить полноценные контейнеры сразу одновременно. Нужен некоторый минимальный инструмент для начальной подготовки общего окружения и пространства имён. Поэтому появился специально облегчённый образ, выполняющий минимум необходимой функциональности.

Обычно размер pause-образа составляет буквально несколько мегабайт, поскольку его задача крайне ограниченна.

### Strategy for deployments
Rolling update 
- Новые Pod'ы создаются последовательно.
- Старые Pod'ы удаляются поэтапно.
- Можно задать максимальное количество временно избыточных (maxSurge) и отключаемых (maxUnavailable) Pod'ов.
Recreate 
- Сначала прекращаются все работающие Pod'ы.
- Только потом разворачиваются новые Pod'ы.
Blue Green deployment
- Развёрнута первая стабильная версия («синяя»).
- Параллельно развёртывается вторая версия («зелёная») отдельно.
- Производится постепенное тестирование и верификация зелёной версии.
- Переключение пользователей осуществляется сменой маршрутов в сервисе или Ingress-е.
Canary Release (Тестирование малых групп пользователей)
- Постепенное увеличение доли пользователей, переходящих на новую версию.
- Тестирование небольшой группы пользователей до полного перевода всех пользователей на новую версию.
A/B Testing (Тестирование двух версий параллельно)
- Одновременное размещение обеих версий и выбор лучшей по результатам теста.

### Deployment, daemonset, statefulset
Deployment → для stateless‑сервисов с гибким масштабированием.

StatefulSet → для stateful‑приложений с уникальными подами и постоянными данными.

DaemonSet → для системных агентов, которые должны работать на каждой ноде.

StatefulSet
Назначение: управление статусными (stateful) приложениями, где важны:
уникальность подов;
упорядоченность операций;
стабильные сетевые идентификаторы.

Ключевые особенности:
Поды получают уникальные имена (web-0, web-1 и т. д.).
Стабильные идентификаторы: каждый под имеет:
постоянное DNS‑имя (web-0.my-service.namespace.svc.cluster.local);
закреплённый PVC (PersistentVolumeClaim) даже при переезде на другую ноду.
Упорядоченное масштабирование: добавление/удаление подов по порядку (0 → 1 → 2).
Упорядоченные обновления: по одному поду, с контролем состояния.
Примеры: базы данных (MySQL, PostgreSQL), кластеры (Kafka, Elasticsearch).
Когда использовать: когда приложение требует:
сохранения локальных данных;
чёткой нумерации подов;
предсказуемой сетевой идентификации.

# Taint and toleration 
Taint(зараза)
Официальное значение: метка на ноде, которая отпугивает поды.
Как работает: если на ноде стоит taint, по умолчанию на неё нельзя запланировать поды — они «боятся» этой ноды.
yaml
taints:
  - key: "dedicated"
    value: "gpu"
    effect: "NoSchedule"
Эффекты (самые частые):

NoSchedule — не планировать поды без toleration.
PreferNoSchedule — стараться не планировать, но можно.
NoExecute — выгнать уже работающие поды (если у них нет toleration).

Toleration («противоядие»)
Официальное значение: разрешение для пода «терпеть» определённый taint.
Как работает: если у пода есть toleration, соответствующий taint ноды его не отпугивает — под может быть запланирован.
yaml
tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"

# Metric server
Что такое Metrics Server
Metrics Server — это компонент Kubernetes, который:
собирает базовые метрики использования ресурсов (CPU, память) с нод и подов;
предоставляет их через Kubernetes API (metrics.k8s.io/v1beta1);
служит источником данных для:
команды kubectl top;
Horizontal Pod Autoscaler (HPA);
других инструментов мониторинга.
Как работает:
Под Metrics Server запускается в кластере (обычно в namespace kube-system).
Периодически опрашивает Kubelet на каждой ноде через API.
Агрегирует данные и выкладывает их в API Kubernetes.
kubectl top и HPA запрашивают метрики через этот API.

Установите Metrics Server (если отсутствует)
Официальный манифест:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl get pods -n kube-system -l k8s-app=metrics-server
kubectl get deployment metrics-server -n kube-system

# Liveness and Readiness probes
initialDelaySeconds
Это параметр в настройках livenessProbe и readinessProbe, который задаёт:
сколько секунд Kubernetes должен подождать после запуска контейнера, прежде чем начать выполнять проверки probe.
Зачем нужен:
Дать приложению время на «разогрев» (загрузка конфигураций, подключение к БД, инициализация кешей).
Избежать ложных срабатываний («приложение не отвечает!»), пока оно ещё стартует.

containers:
- name: my-app
  image: my-image:latest
  livenessProbe:
    httpGet:
      path: /health
      port: 8080
    initialDelaySeconds: 30  # ждать 30 сек после старта контейнера
    periodSeconds: 10     # потом проверять каждые 10 сек
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 20  # ждать 20 сек перед первой проверкой
    periodSeconds: 5      # потом проверять каждые 5 сек

initialDelaySeconds: задержка перед первой проверкой.
periodSeconds: интервал между проверками (по умолчанию 10 сек).
timeoutSeconds: сколько ждать ответа probe (по умолчанию 1 сек).
successThreshold: сколько успешных проверок нужно для «выздоровления» (по умолчанию 1).
failureThreshold: сколько неудач до срабатывания (для liveness — перезапуск; для readiness — исключение из балансировки).

# Type of services k8s

ClusterIP
для внутренних микросервисов (БД, кэш, бэкенды);
когда не нужен внешний доступ или потом обеспечить доступ через ingress/harproxy 

nodePort:
для быстрого тестирования;
если нет балансировщика нагрузки.

LoadBalancer:
для продакшен‑приложений с внешним доступом;
когда нужна высокая доступность и автоматическая балансировка.
(Даже для LoadBalancer создаётся ClusterIP (внутри кластера), балансировщик не общается напрямую с подами — только через ClusterIP и kube‑proxy.)

ExternalName:
чтобы сослаться на внешний сервис (например, SaaS) через Kubernetes‑DNS;
для миграции (временно направить трафик вовне).

Pod Topology Spread Constraints — это механизм в Kubernetes, позволяющий управлять распределением подов по узлам кластера с учётом топологии инфраструктуры (зоны доступности, регионы, ноды и др.). Цель — обеспечить отказоустойчивость и балансировку нагрузки.

Зачем нужны
Высокая доступность. Равномерное распределение подов по зонам/нодам снижает риск потери сервиса при сбое одного узла или зоны.

Оптимизация ресурсов. Предотвращает «скучивание» подов на одних нодах и недоиспользование других.

Учёт доменов отказов. Гарантирует, что поды не сосредоточены в одном физическом сегменте (например, в одной зоне AWS/GCP).

# Cordon, drain 
Запретите планирование новых подов (cordon)
kubectl cordon <node-name>
Нода останется в кластере, но планировщик (kube-scheduler) не будет размещать на ней новые поды.
Статус ноды изменится на Ready,SchedulingDisabled.
Выведите поды с ноды (drain)
kubectl drain <node-name> \
  --ignore-daemonsets \
  --delete-emptydir-data
--ignore-daemonsets — не трогать поды из DaemonSet (обычно системные: мониторинг, сеть, логгинг).
--delete-emptydir-data — удалить данные из emptyDir (если допустимо).

Kubernetes:
пометит поды как Terminating;
перенесёт их на другие ноды (если есть ресурсы и правила PodDisruptionBudget позволяют);
сохранит количество реплик (за счёт ReplicaSet/Deployment).

Важно:
Если есть PodDisruptionBudget, убедитесь, что он позволяет вывести поды.
Для подов с hostPath или stateful‑приложений (БД) может потребоваться ручная миграция данных.

# StatefulSet
StatefulSet: простыми словами
StatefulSet — это контроллер в Kubernetes для управления stateful‑приложениями (теми, что «помнят» своё состояние — например, базы данных, очереди сообщений). В отличие от Deployment (который подходит для «безликих» сервисов), StatefulSet гарантирует стабильные идентификаторы и постоянное хранилище для каждого пода.

Ключевые свойства (простыми словами)
Уникальные, стабильные имена подов

Поды получают имена вида my-app-0, my-app-1, my-app-2 и т. д.

Имена не меняются при перезапуске или пересоздании.

Это важно, например, для кластерных БД, где ноды должны «знать» друг друга по имени.

Стабильные сетевые идентификаторы

Для каждого пода создаётся DNS‑запись в формате:
<pod-name>.<service-name>.<namespace>.svc.cluster.local
Например: my-app-0.my-service.default.svc.cluster.local.

Приложения могут подключаться к конкретным подам по этим именам (например, для настройки репликации).
Постоянные объёмы хранения (PersistentVolume)
Каждый под получает свой собственный PV (через volumeClaimTemplates).
При пересоздании пода он «подхватывает» тот же PV — данные не теряются.
Пример: если my-app-0 падал, после перезапуска он будет использовать тот же диск с данными.
Последовательное масштабирование и обновление
Масштабирование: новые поды создаются строго по порядку (0 → 1 → 2).
Удаление: поды удаляются в обратном порядке (2 → 1 → 0).
Обновление: применяется пошагово (сначала 0, потом 1 и т. д.), чтобы не нарушить работу кластера.

Гарантия порядка при запуске
Поды запускаются строго по порядку: my-app-0 → my-app-1 → my-app-2.
Это нужно, например, чтобы первичный узел БД стартовал раньше вторичных.
Сохранение привязок при переезде
Если под переезжает на другую ноду (из‑за сбоя), он:
сохраняет своё имя (my-app-1);
получает тот же PV;
остаётся доступен по тому же DNS‑имени.
Поддержка кластерных приложений

Идеален для:
баз данных (MySQL, PostgreSQL, MongoDB, Cassandra);
очередей сообщений (Kafka, RabbitMQ);
распределённых хранилищ (Ceph, MinIO).
В таких системах важно, чтобы ноды «узнавали» друг друга и сохраняли данные

# QoS
Quality of Service (QoS) в Kubernetes — это механизм приоритизации подов при нехватке ресурсов на ноде. Он определяет, какие поды будут вытеснены (evicted) в первую очередь, если нода исчерпает память или CPU.
Три уровня QoS
В Kubernetes существует три класса QoS (в порядке убывания приоритета):
Guaranteed (гарантированный)
Burstable (с возможностью превышения лимитов)
BestEffort (наилучшее усилие)
Как определяется класс QoS
Класс присваивается автоматически на основе параметров resources в спецификации пода.

Guaranteed (высший приоритет)
Условия:
Для всех контейнеров пода заданы и requests, и limits для CPU и памяти.
Значения requests и limits совпадают для каждого контейнера.

Burstable (средний приоритет)
Условия:
Хотя бы для одного контейнера заданы requests или limits (но не оба параметра совпадают).
Либо requests ≠ limits.

BestEffort (низший приоритет)
Условия:
Ни для одного контейнера не заданы requests или limits.

Note:
При нехватке памяти kubelet сначала вытесняет поды BestEffort, затем Burstable, и только в крайнем случае — Guaranteed.

# Overcommit
Overcommit в Kubernetes — это ситуация, когда суммарные заявленные (requests) или максимально допустимые (limits) ресурсы для подов превышают физические возможности ноды (CPU, память).
Как это работает
requests — минимальные гарантированные ресурсы для пода. Планировщик (kube-scheduler) учитывает их при размещении пода на ноде.
limits — максимальный «потолок» ресурсов, который может использовать под.
Overcommit возникает, если:
сумма requests всех подов > ресурсов ноды;
или сумма limits > ресурсов ноды (даже если requests укладываются).

Overcommit в Kubernetes — это ситуация, когда суммарные заявленные (requests) или максимально допустимые (limits) ресурсы для подов превышают физические возможности ноды (CPU, память).

Как это работает
requests — минимальные гарантированные ресурсы для пода. Планировщик (kube-scheduler) учитывает их при размещении пода на ноде.

limits — максимальный «потолок» ресурсов, который может использовать под.

Overcommit возникает, если:

сумма requests всех подов > ресурсов ноды;

или сумма limits > ресурсов ноды (даже если requests укладываются).

Почему это возможно
Приложения редко используют максимум. Многие сервисы потребляют меньше limits, поэтому система «рискует» и размещает больше подов, чем физически может обслужить.

Динамическая нагрузка. Ресурсы нужны пиково — в остальное время нода простаивает.

Экономическая выгода. Позволяет плотнее «упаковать» ноды и снизить затраты на инфраструктуру.

Риски overcommit
Вытеснение подов (eviction)

При нехватке памяти kubelet начинает убивать поды (сначала BestEffort, потом Burstable).

Код ошибки: OOMKilled (Exit Code 137).

Троттлинг CPU

Если поды превышают limits по CPU, их выполнение замедляется.

Приложение тормозит, растёт время отклика.

Нестабильность ноды

Перегрузка памяти/CPU → сбои системных процессов (kubelet, container runtime).

Нода может перейти в статус NotReady.

Проблемы с планированием

Планировщик видит, что нода «занята» по requests, и не размещает новые поды.

Даже если физически ресурсы есть, поды зависают в Pending.

Каскадные сбои

Вытеснение одного пода → нагрузка на другие → цепная реакция.

Пример безопасного overcommit
Сценарий:

Нода: 8 CPU, 32 GB RAM.

Поды: 10 подов с requests: 500m CPU, 1GiB RAM.

Сумма requests: 5 CPU, 10 GB RAM (укладывается в ноду).

limits: 1 CPU, 2 GB RAM на под (сумма превышает ноду, но реально поды редко достигают лимитов).

Результат:

Планировщик разместит поды, так как requests в норме.

При умеренной нагрузке всё работает.

Если какой‑то под начнёт потреблять limits, kubelet вытеснит менее приоритетные поды.

Итог
Overcommit — это компромисс между эффективностью и надёжностью.

Плюсы: экономия ресурсов, высокая плотность размещения.

Минусы: риск сбоев при пиковой нагрузке.

Правило:

«Настраивайте requests честно, limits — с разумным запасом, а мониторинг — непрерывно».

# Kube‑controller‑manager
Главный компонент — kube‑controller‑manager (на control plane). Он содержит Node Controller, который:
следит за статусом нод;
реагирует на пропажи/возвращения нод;
инициирует эвакуацию подов при падении ноды.

Нода (kubelet) отправляет два типа сигналов:

Статус ноды (Node Status)

kubelet регулярно (по умолчанию — каждые 10 сек) обновляет объект Node в API Server.

В статусе указывается:

Ready (готов/не готов);

условия (Conditions: MemoryPressure, DiskPressure, PIDPressure, Ready);

адреса, ёмкость, выделенные ресурсы.

Heartbeat (пульс)

kubelet шлёт heartbeat через lease‑ресурс (API: coordination.k8s.io/v1 Leases).

По умолчанию — каждые 10 сек, тайм‑аут — 40 сек.

Если lease не обновляется, Node Controller считает ноду пропавшей.

Кто и как опрашивает
kube‑controller‑manager (Node Controller)
Периодически (каждые 5 сек по умолчанию) проверяет:
актуальность NodeStatus;
свежесть Lease.
Если оба сигнала пропали, нода переводится в статус NotReady.
kube‑apiserver
Принимает обновления статуса от kubelet.
Хранит актуальное состояние нод в etcd.
kubelet на ноде
Сам инициирует отправку статуса и lease (не ждёт опроса).
Проверяет локальное состояние (память, диск, PID) и включает соответствующие Conditions.

internalTrafficPolicy
internalTrafficPolicy — это поле в Kubernetes‑сервисе (Service), которое управляет маршрутизацией трафика внутри кластера (от подов к сервису). Оно определяет, на какие ноды могут перенаправляться запросы к сервису.

Зачем нужно
По умолчанию Kubernetes балансирует трафик ко всем нодам, где есть поды сервиса. Это иногда:

увеличивает задержки (трафик может идти через «лишние» ноды);

расходует полосу (дополнительные пересылки между нодами);

снижает безопасность (трафик выходит за пределы локальной ноды).

# InternalTrafficPolicy 
позволяет ограничить маршрутизацию только теми нодами, где реально запущены поды сервиса.

Доступные значения
Cluster (поведение по умолчанию)

Трафик распределяется по всем нодам кластера, где есть эндпоинты сервиса.

Kube‑proxy выбирает случайную ноду из списка и перенаправляет запрос.

Подходит для равномерной нагрузки, но может создавать «лишние» переходы.

Local

Трафик направляется только на ноду, где запущен поды сервиса.

Если на текущей ноде нет подходящих подов — запрос отбрасывается (или возвращается ошибка 503, зависит от реализации).

Снижает задержки и межнодовый трафик, повышает безопасность.

internalTrafficPolicy и externalTrafficPolicy — это настройки Kubernetes Service, влияющие на то, каким образом трафик распределяется внутри кластера и снаружи его границ.

▌ Internal Traffic Policy

Определяет поведение сервисов Kubernetes относительно трафика, поступающего изнутри кластера:

- Cluster: весь внутренний трафик направляется случайному pod'у сервиса независимо от его местоположения в узлах (default).
- Local: запросы направляются только на тот узел, где запущен соответствующий pod. Если локального pod'а нет, сервис вернёт ошибку.

Эта настройка полезна, когда важна минимизация сетевых задержек или необходима высокая производительность обработки запросов, особенно в высоконагруженных системах.

▌ External Traffic Policy

Управляет поведением балансировки нагрузки внешнего трафика (трафик извне кластера):

- Cluster: внешний трафик поступает равномерно ко всем рабочим узлам кластера, даже если там нет подходящего pod'а. Это улучшает отказоустойчивость, но снижает производительность, так как возможна дополнительная нагрузка на узлы.
- Local: внешние запросы перенаправляются только на те узлы, где запущены подходящие pod'ы. Такой режим обеспечивает меньшую нагрузку на сеть и лучшие показатели производительности, однако требует тщательного планирования распределения pod'ов по узлам.

Использование этих настроек позволяет оптимизировать работу приложений в зависимости от специфики инфраструктуры и требований к нагрузке и производительности системы.


##########################################################################################################################################
# Gateway API


##########################################################################################################################################
Distroless images — это особый вид контейнерных образов Docker, специально оптимизированных для минимизации размера образа, повышения производительности и уменьшения поверхности атаки приложений, работающих внутри контейнера.

▌ Основные особенности Distroless-образов:

1. Минимализм:

   - Содержат только необходимое ПО и библиотеки для запуска приложения.
   - Избавлены от стандартных утилит и пакетов операционной системы (например, bash, curl, wget).

2. Безопасность:

   - Уменьшение числа компонентов снижает риск уязвимости.
   - Образы содержат меньше метаданных и файлов, облегчающих аудит и защиту.

3. Производительность:

   - Быстро загружаются благодаря малому размеру.
   - Экономия ресурсов хост-машины и облака.

4. Компактность:

   - Типичный размер обычного дистрибутива Linux составляет десятки мегабайт, тогда как distroless образ часто занимает всего несколько мегабайт.

##########################################################################################################################################
# GIT 
### How to move commit from branch to another branch 
Шаг 1: Узнай SHA-хэш нужного коммита.
Для этого используй команду:
git log
Выбери нужный коммит по описанному сообщению или дате, запомни его SHA-хэш (это длинная строка типа abcdef123456...).
Шаг 2: Переключись на нужную ветку, куда хочешь перенести коммит.
git checkout feature
Шаг 3: Переноси коммит в новую ветку с помощью команды cherry-pick.
git cherry-pick abcdef123456
Git применит выбранный коммит в твою текущую ветку.

### With rebase 
Перемещение коммита через rebase
Этот способ подходит, если надо переместить целую серию коммитов или перестроить историю ветки.

### About rebase 
Rebase — это способ перестройки истории изменений в репозитории Git таким образом, чтобы твоя ветвь выглядела так, будто твои изменения были сделаны поверх самой свежей версии основного кода.
Другими словами, представь, что ты работаешь над своей задачей, создавая новую ветку от главной (main). Пока ты вносишь свои изменения, кто-то другой параллельно обновил главную ветку (main), добавив туда новые коммиты.
Теперь у тебя есть два варианта синхронизации твоей ветки с изменениями главной ветви:
1. Merge (слияние) — объединить свою ветку с главным путём добавления нового специального коммита ("merge-commit"), который объединяет обе ветки.
2. Rebase (перестройка) — переписать историю твоей ветки так, чтобы твои изменения казались сделанными после свежих изменений в главной ветке.


##########################################################################################################################################
# Networks
| Устройство  |      IP      |     Маска     |
| ----------- | ------------ | ------------- |
| Компьютер A | 192.168.1.10 | 255.255.255.0 |
| Компьютер B | 192.168.1.20 | 255.255.255.0 |
Компьютеры находятся в одной подсети (192.168.1.x), поэтому взаимодействие возможно напрямую, даже если шлюз не настроен. (через ARP-протокол, определяя MAC-адреса друг друга)

##########################################################################################################################################
# API Methods
Kinds of methods and how does it works?
GET - Метод GET используется для получения ресурсов. Это самый простой метод, предназначенный для извлечения данных с сервера. Основные характеристики метода GET:
- Запросы передаются через адресную строку браузера (URL), включая параметры в строке запроса.
- Данные видны пользователям и индексируются поисковиками.
- Безопасен и идемпотентен (не меняет состояние ресурса).
- Ограничение на длину строки запроса (~2048 символов в зависимости от реализации браузером).

POST - Метод POST предназначен для отправки данных на сервер для обработки. Обычно используется для передачи больших объёмов данных, формы, загрузки файлов и прочих операций, изменяющих состояние системы. Особенности метода POST:
- Данные отправляются внутри тела запроса, скрытые от пользователей.
- Нет ограничений на размер передаваемых данных.
- Может изменять состояние на стороне сервера (например, создавать новые записи).
- Индексация поисковиками невозможна

PUT - Используется для обновления существующего ресурса или создания нового ресурса с указанным путём. Если ресурс существует, он обновляется, иначе создаётся новый. Отличается от POST, так как имеет строгое назначение пути ресурса.

DELETE - Удаляет указанный ресурс с сервера. Этот метод считается безопасным и идемпотентным, поскольку повторные запросы приводят к одному и тому же результату (если ресурс удалён ранее, последующие DELETE-запросы завершатся успешно).

PATCH - Применяется для частичного изменения ресурса. Полезен, когда нужно обновить лишь некоторые поля ресурса, не затрагивая весь объект целиком.

HEAD - Похож на метод GET, но возвращает только заголовки ответа, без самого содержимого. Используется для проверки доступности ресурса, получения метаданных (размер файла, тип контента и т.п.).

OPTIONS - Запрашивает доступные методы для указанного ресурса. Часто применяется для тестирования API-интерфейсов и кросс-доменных запросов.

##########################################################################################################################################
# Databases
PostgresSQL about sunc and async
## Синхронная запись данных означает, что приложение ожидает подтверждения завершения операции записи от базы данных перед продолжением своей работы. Этот подход гарантирует высокую степень надежности, так как клиент точно знает, была ли операция успешной или произошла ошибка.

Преимущества синхронной записи:
- Высокая надёжность: транзакция гарантированно выполнена успешно либо откатана в случае сбоя.
- Легко отслеживать состояние операций и управлять ими.

Недостатки:
- Потенциально низкая производительность, особенно при большом количестве параллельных операций, так как каждая операция должна дождаться своего подтверждения.

Примеры ситуаций, когда предпочтительна синхронная запись:
- Финансовые транзакции, где критически важна целостность данных.
- Логирование важных действий пользователей (например, регистрация аккаунта).

## Асинхронная запись позволяет приложению продолжить выполнение сразу же после отправки команды записи в базу данных, не дожидаясь её завершения. В этом режиме клиент отправляет команду на запись и продолжает работу дальше, не ожидая результата.

Преимущества асинхронной записи:
- Высокое быстродействие и масштабируемость: система не простаивает, ожидая результатов каждой отдельной операции.
- Повышенная пропускная способность, особенно при больших объемах записей.

Недостатки:
- Сложнее контролировать успех операции записи: если база данных не подтвердила успешность операции, приложение может оказаться в неопределённом состоянии.
- Возможность потери данных при отказе оборудования или сети.

Примеры ситуаций, когда оправдан выбор асинхронной записи:
- Сбор статистики и аналитики, где потеря отдельных записей менее критична.
- Операции, создающие большое количество мелких изменений в базе данных, которые легко восстанавливаются вручную.
## RabbitMQ 
How to check that consumer consume messages just check queue in rabbitmq
if you have more than 1 host may be 5, so you can use script to watch all queues pn all hosts

## WAL 
WAL (Write Ahead Log) — это метод журналирования изменений в базе данных, используемый для повышения надежности и восстановления базы данных после сбоев. Суть метода заключается в следующем:
Как работает Write Ahead Logging?
При любом изменении данных (например, вставке, обновлении или удалении записей):
1. Запись журнала: Прежде чем изменения будут записаны непосредственно в базу данных, система сначала фиксирует эти изменения в специальном журнале транзакций (лог-файл). Это делается заранее ("write ahead"), отсюда и название метода.
2. Подтверждение транзакции: После записи всех необходимых операций в журнал, база данных подтверждает успешное завершение транзакции клиенту. Только после подтверждения транзакции начинается запись данных в основную таблицу.
3. Фиксация данных: Данные постепенно переносятся из журнала в саму базу данных в фоновом режиме либо синхронно в зависимости от настроек системы.

##########################################################################################################################################
# Ansible
Идемпотентность: Ansible не переустанавливает пакет, если он уже есть. В Go нужно проверять вручную (например, dpkg -l | grep pkg).

# ##########################################################################################################################################
# Docker 
COPY vs ADD 
### ADD
- Что делает: Копирует файлы и директории из локальной файловой системы хоста внутрь контейнера.
- Особенности:
  - Может скачивать удалённые архивы (tar.gz) непосредственно из сети по ссылке и распаковывать их автоматически внутри образа.
  - Поддерживает автоматическое извлечение архивов (*.tar, *.tar.gz, *.tgz), если копируется файл с расширением .tar.
  - Если файл имеет расширение .tar, .tar.gz или .tgz, Docker автоматически распакует его в целевой каталог.
### example:
ADD https://example.com/file.tar /target/directory/
Note: Add имеет возможность еще ходить куда-то на внешние ресурсы и стягивать оттуда данные

### COPY
- Что делает: Точно такая же операция, как и ADD, но без поддержки загрузки файлов из Интернета и автоматического распаковки архивов.
- Особенности:
  - Работает быстрее и проще, поскольку не поддерживает загрузку удалённых ресурсов и распаковку.
  - Рекомендуется использовать именно её, если вам не нужны дополнительные возможности ADD. Это улучшает читаемость Dockerfile и снижает вероятность ошибок.

### CMD vs ENTRYPOINT
### CMD
- Что делает: Предоставляет команду по умолчанию, которая запускается контейнером при старте.
- Особенности:
  - Является аргументом по умолчанию для команды, указанной в ENTRYPOINT.
  - Можно переопределять командой запуска контейнера, передавая ей новые аргументы.
  - Удобно использовать, если приложение допускает гибкость в параметрах запуска.

CMD ["executable","param1","param2"] # JSON format (рекомендуемый)
CMD command param1 param2              # Shell format
### example:
CMD ["python", "app.py"] or CMD ["./master-npd"] or CMD ["./manager-ns"]

### ENTRYPOINT
- Что делает: Определяет исполняемый файл, который обязательно запустится при старте контейнера.
- Особенности:
  - Используется вместе с CMD: команда из CMD становится аргументами для команды из ENTRYPOINT.
  - Полезно применять для приложений, которым необходимы фиксированные параметры запуска.
  - Переопределение возможно только полным перезапуском всего процесса (например, использование другого скрипта).

ENTRYPOINT ["executable", "param1", "param2"] # JSON format (рекомендуемый)
ENTRYPOINT executable param1 param2           # Shell format
### example:
ENTRYPOINT ["python"]
CMD ["app.py"]
CMD задаёт параметры по умолчанию, которые легко переопределяются, а ENTRYPOINT определяет основной исполняемый процесс, который сложно изменить при запуске контейнера.

### another example
Представьте себе сценарий, где мы хотим запустить скрипт Python с разными параметрами каждый раз при запуске контейнера. Например, у вас есть разные режимы работы приложения, зависящие от передаваемых аргументов.
Допустим, ваше приложение называется run.py, и оно принимает два разных режима работы:
- Режим чтения данных: read_data
- Режим записи данных: write_data

Example Dockerfile
FROM python:3.8-slim
WORKDIR /app
# Скопируем исходники
COPY . .
# Укажите стандартную команду для запуска
CMD ["python", "run.py", "read_data"]

Теперь при запуске контейнера:
- По умолчанию выполняется python run.py read_data, потому что это записано в команде CMD.
- Но если нам нужно другое поведение, скажем, запись данных, мы можем передать новый аргумент при запуске контейнера:
docker run my_image python run.py write_data <<<<< change read to write !!! when try to run our application
Здесь важно отметить, что команда из CMD изменяется прямо при запуске контейнера.

Например, пусть у нас есть веб-приложение Flask, которое должно стартовать сервер и передавать любые дополнительные параметры, как IP-адрес или номер порта.

Dockerfile может выглядеть следующим образом:

dockerfile

FROM python:3.8-slim

WORKDIR /app

# Скопируем исходники
COPY . .

# Установим пакет
RUN pip install flask

# Используем ENTRYPOINT для запуска сервера
ENTRYPOINT ["flask", "run"] <<< неизменна и всегда выполнятеся 

# Передадим стандартный адрес и порт
CMD ["--host=0.0.0.0", "--port=5000"] <<<  а уже через CMD  можно изменить аргументы ну или Deployment 
При таком подходе:

- Когда вы запускаете контейнер без дополнительных аргументов, Docker выполнит flask run --host=0.0.0.0 --port=5000.
- Однако если вам нужно задать нестандартный хост или порт, вы можете сделать это прямо при запуске контейнера:

docker run my_image --host=127.0.0.1 --port=8000
И вот ключевое отличие:

- Команда из ENTRYPOINT неизменна и должна обязательно выполняться.
- Аргумент из CMD меняется динамически при каждом запуске контейнера.

Так что итоговая последовательность действий выглядит так:
- Сначала выполняется команда из ENTRYPOINT, затем ей передаются аргументы из CMD.
▌ Итог:
- Используйте CMD, если хотите иметь возможность менять команду при запуске контейнера.
- Применяйте ENTRYPOINT, если ваша программа должна гарантированно начать работу с конкретного исполняемого файла или команды, и дальнейшие изменения возможны лишь в виде дополнительных аргументов.

### Example with my Dockerfile
FROM registry.apps.k8s.<my-registry>/golang/golang:1.24.0   AS builder
WORKDIR /app
COPY go.mod go.sum ./
COPY vendor/ ./vendor/
COPY . .
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o master-npd main-npd/master-npd/cmd/main.go

FROM registry.apps.k8s.<my-registry>/alpine/alpine:3.22.1
WORKDIR /app
COPY --from=builder /app/master-npd /app/
RUN  chmod u+x master-npd && mkdir /certs  /files
CMD ["./master-npd"]
### Здесь CMD лучше сменить на ENTRYPOINT так как кроме своего binary я более ничего не запускаю 

##########################################################################################################################################
# ArgoCD
Background and Foreground 
Argo CD различает два режима удаления ресурсов при выполнении синхронизаций: Foreground Deletion и Background Deletion. Рассмотрим каждый режим детально:

▌ Foreground Deletion (Удаление переднего плана)

Режим foreground deletion подразумевает удаление ресурсов синхронно. Argo CD ждёт, пока зависящие ресурсы завершатся естественным образом, прежде чем удалить родительский ресурс.

Этот метод полезен, когда существуют зависимости между ресурсами, и требуется гарантия последовательного удаления, чтобы предотвратить нежелательные побочные эффекты. Например, если у вас есть Deployment и связанный с ним Service, Argo CD дождётся, пока все Pod'ы перестанут обслуживать трафик, прежде чем удалять Deployment.

Преимущества:

- Последовательное удаление, снижающее вероятность возникновения ошибок.
- Полностью управляемое поведение.

Недостатки:

- Может занять больше времени, особенно если ресурсы имеют долгую процедуру освобождения.

▌ Background Deletion (Удаление заднего плана)

Режим background deletion позволяет быстро удалить ресурсы, передав очистку зависимых ресурсов соответствующему контроллеру Kubernetes. Удаление производится асинхронно, без ожидания окончания всех подчинённых ресурсов.

Преимуществом этого метода является скорость удаления. Недостатком — отсутствие контроля над порядком удаления ресурсов, что может привести к временным ошибкам, если зависимые ресурсы ещё работают.

Преимущества:

- Быстрое удаление.
- Подходит для простых сценариев, где порядок удаления не критичен.

Недостатки:

- Возможны временные ошибки, вызванные некорректным порядком удаления.
When you try to delete via WEBUI argo as you about how to delete it in Foreground or Background
